{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separable segmentation\n",
    "\n",
    "> The idea is to use a set of grouped convolution to perform the final segmentation in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "> We will be using CityScapes as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different training, validation and test subsets are divided in their correspondent folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = Path(\"/media/disk/databases/dn_segmentation/cityscapes\")\n",
    "path_train = path_root / \"train_data\"\n",
    "path_val = path_root / \"validation_data\"\n",
    "path_test = path_root / \"test_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a set of generators to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_path_from_img(path_img):\n",
    "    path_label = str(path_img).split(\"_\")\n",
    "    path_label[-1] = \"gtFine_color.png\"\n",
    "    path_label = \"_\".join(path_label)\n",
    "    path_label = path_label.replace(\"images\", \"labels\")\n",
    "    path_label = Path(path_label)\n",
    "    return path_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    for path_img in (path_train/\"images\").glob(\"*.png\"):\n",
    "        path_label = get_label_path_from_img(path_img)\n",
    "\n",
    "        ## Load the images\n",
    "        img = cv2.imread(str(path_img))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, dsize=(256,128))\n",
    "        label = cv2.imread(str(path_label))\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.resize(label, dsize=(256,128), interpolation=cv2.INTER_NEAREST)\n",
    "        yield img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_gen():\n",
    "    for path_img in (path_val/\"images\").glob(\"*.png\"):\n",
    "        path_label = get_label_path_from_img(path_img)\n",
    "\n",
    "        ## Load the images\n",
    "        img = cv2.imread(str(path_img))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, dsize=(256,128))\n",
    "        label = cv2.imread(str(path_label))\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.resize(label, dsize=(256,128), interpolation=cv2.INTER_NEAREST)\n",
    "        yield img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(train_gen())\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(9,4))\n",
    "\n",
    "axes[0].imshow(a)\n",
    "axes[1].imshow(b)\n",
    "for ax in axes: ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the labels into a sparse representation\n",
    "\n",
    "> The labels are loaded as colors, but now we need to turn them into sparse representations to be able to train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will be obtaining the individual colors in the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([(0, 0, 0), (111, 74, 0), (81, 0, 81), (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), \n",
    "                   (102, 102, 156), (190, 153, 153), (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (250, 170, 30), (220, 220, 0), (107, 142, 35), \n",
    "                   (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), ( 0, 0, 142), ( 0, 0, 70), (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), \n",
    "                   (119, 11, 32), (0, 0, 142)], dtype = np.int32)\n",
    "colors = set([tuple(c) for c in colors])\n",
    "len(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a dictionary mapping each color into one integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color2label = {color:i for i,color in enumerate(colors)}\n",
    "label2color = {i:color for i,color in enumerate(colors)}\n",
    "len(color2label)#, label2color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can convert the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label(label):\n",
    "    new_label = np.empty(shape=label.shape[:-1])\n",
    "    for i, j in product(range(label.shape[0]), range(label.shape[1])):\n",
    "        color = label[i,j]\n",
    "        new_label[i,j] = color2label[tuple(color)]\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label_one_hot(label):\n",
    "    new_label = np.zeros(shape=(*label.shape[:-1], len(colors)))\n",
    "    for i, j in product(range(label.shape[0]), range(label.shape[1])):\n",
    "        color = label[i,j]\n",
    "        idx = color2label[tuple(color)]\n",
    "        new_label[i,j,idx] = 1\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_sparse(label):\n",
    "    \"\"\"Turns a one-hot encoded label into a sparse one.\"\"\"\n",
    "    return np.argmax(label, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    for path_img in (path_train/\"images\").glob(\"*.png\"):\n",
    "        path_label = get_label_path_from_img(path_img)\n",
    "\n",
    "        ## Load the images\n",
    "        img = cv2.imread(str(path_img))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, dsize=(256,128))\n",
    "        label = cv2.imread(str(path_label))\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.resize(label, dsize=(256,128), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        ## Prepare the labels\n",
    "        # label = prepare_label(label)\n",
    "        label = prepare_label_one_hot(label)\n",
    "        # label = tf.expand_dims(label, -1)\n",
    "        \n",
    "        yield img/255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_gen():\n",
    "    for path_img in (path_val/\"images\").glob(\"*.png\"):\n",
    "        path_label = get_label_path_from_img(path_img)\n",
    "\n",
    "        ## Load the images\n",
    "        img = cv2.imread(str(path_img))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, dsize=(256,128))\n",
    "        label = cv2.imread(str(path_label))\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.resize(label, dsize=(256,128), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        ## Prepare the labels\n",
    "        # label = prepare_label(label)\n",
    "        label = prepare_label_one_hot(label)\n",
    "        # label = tf.expand_dims(label, -1)\n",
    "        \n",
    "        yield img/255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(train_gen())\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = tf.data.Dataset.from_generator(\n",
    "                            train_gen,\n",
    "                            output_signature=(\n",
    "                                tf.TensorSpec(a.shape, tf.float32),\n",
    "                                tf.TensorSpec(b.shape, tf.int32),\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_val = tf.data.Dataset.from_generator(\n",
    "                            val_gen,\n",
    "                            output_signature=(\n",
    "                                tf.TensorSpec(a.shape, tf.float32),\n",
    "                                tf.TensorSpec(b.shape, tf.int32),\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "> The important part is going to be at the last part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"pre_last_activation\": \"sigmoid\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"segmentation_test\",\n",
    "           name=\"Sigmoid\",\n",
    "           config=config)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.Input(shape=(1024,2048,3))\n",
    "inputs = tf.keras.Input(shape=(128, 256,3))\n",
    "# resize = layers.Resizing(128, 256)\n",
    "conv1_e = layers.Conv2D(32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")\n",
    "conv2_e = layers.Conv2D(64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")\n",
    "conv3_e = layers.Conv2D(128, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")\n",
    "pooling = layers.MaxPooling2D(2)\n",
    "upsampling = layers.UpSampling2D(2)\n",
    "conv2_d = layers.Conv2D(32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")\n",
    "conv1_d = layers.Conv2D(len(color2label)*3, kernel_size=5, strides=1, padding=\"same\", activation=config.pre_last_activation)\n",
    "classifier = layers.Conv2D(len(color2label), kernel_size=1, groups=len(color2label), strides=1, padding=\"same\", activation=\"softmax\")\n",
    "\n",
    "## Encoder\n",
    "# inputs_resized = resize(inputs)\n",
    "# output_1e = conv1_e(inputs_resized)\n",
    "output_1e = conv1_e(inputs)\n",
    "output_2e = conv2_e(pooling(output_1e))\n",
    "output_3e = conv3_e(pooling(output_2e))\n",
    "\n",
    "##Â Decoder\n",
    "\n",
    "upsampled_3 = upsampling(output_3e)\n",
    "output_2d = conv2_d(tf.concat([output_2e, upsampled_3], axis=-1))\n",
    "upsampled_2 = upsampling(output_2d)\n",
    "output_1d = conv1_d(tf.concat([output_1e, upsampled_2], axis=-1))\n",
    "\n",
    "## Separable classifier\n",
    "output_final = classifier(output_1d)\n",
    "\n",
    "## Model\n",
    "model = tf.keras.Model(inputs, output_final)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              # loss=\"mae\",\n",
    "              metrics=[\"accuracy\",\n",
    "                    #    tf.keras.metrics.IoU(num_classes=len(colors),\n",
    "                    #                         target_class_ids=list(range(len(colors)))),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_masks_wandb(image, \n",
    "                        label, # one-hot encoeded.\n",
    "                        pred, # one-hot encoeded.,\n",
    "                        class_labels,\n",
    "                        ):\n",
    "    label = one_hot_to_sparse(label)\n",
    "    pred = one_hot_to_sparse(pred)\n",
    "    mask_img = wandb.Image(image, masks={\n",
    "        \"predictions\": {\n",
    "            \"mask_data\": pred,\n",
    "            \"class_labels\": class_labels\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"mask_data\": label,\n",
    "            \"class_labels\": class_labels,\n",
    "        }\n",
    "        })\n",
    "    return mask_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationMaskLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to log the segmentation masks properly into WandB.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data: Tuple, # (X, Y) tuple.\n",
    "                 prefix: str = \"\",\n",
    "                 ):\n",
    "        self.data = data\n",
    "        self.prefix = prefix\n",
    "        self.class_labels = {k:str(v) for k,v in label2color.items()}\n",
    "    \n",
    "    def on_epoch_end(self,\n",
    "                     epoch,\n",
    "                     logs=None,\n",
    "                     ):\n",
    "        preds = model.predict(self.data[0], verbose=0)\n",
    "        results = [prepare_masks_wandb(X, Y, pred, self.class_labels) for X, Y, pred in zip(self.data[0], self.data[1], preds)]\n",
    "        wandb.log({f\"{self.prefix}results\":results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dst_train.batch(config.batch_size), epochs=config.epochs,\n",
    "                    validation_data=dst_val.batch(config.batch_size),\n",
    "                    callbacks=[WandbCallback(monitor=\"val_loss\",\n",
    "                                             mode=\"min\",\n",
    "                                             save_weights_only=True,\n",
    "                                             ),\n",
    "                    SegmentationMaskLogger(data=dst_train.batch(8).take(1).get_single_element(), prefix=\"train_\"),\n",
    "                    SegmentationMaskLogger(data=dst_val.batch(8).take(1).get_single_element(), prefix=\"val_\"),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3191470a48c0dde5f5049facf0c01779d6ac0275f8b425c8e052848dbf5d143"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
